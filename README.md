# RAG â€“ LangChain (Ollama + Qdrant + FastAPI/Gradio)

Sistema de **Perguntas & Respostas** sobre documentos (PDF â†’ texto limpo â†’ embeddings â†’ *similarity search* â†’ resposta com LLM), 100% local, com suporte opcional a **GPU (CUDA)**.

## âœ¨ Funcionalidades
- **PreparaÃ§Ã£o de dados**: limpeza/OCR dos PDFs para `.txt` limpos.  
- **IngestÃ£o de dados**: IngestÃ£o dos chunks gea partir dos `.txt` para o **Qdrant**.
- **Chat RAG**: *Similarity search* no **Qdrant** + contexto dos documentos com um **LLM via Ollama**.  
- **Modos**: FastAPI (API REST) e/ou Gradio (UI web).  
- **GPU-ready**: EasyOCR e LLMs acelerados por CUDA (opcional).

## ðŸ§± Arquitetura
```
    PDFs
(data/Original/) â”€â”€â–¶ pdf_clean.py â”€â”€â–¶ .txt (data/Clean/)
                                        â””â”€â–¶ ingest.py â”€â”€â–¶ embeddings (Ollama) â”€â”€â–¶ Qdrant (similarity search)
                                                                                       â–²
                                                                                       â”‚
                                               rag_app.py (FastAPI/Gradio) â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”˜
                                                            â–²
                                                            â”‚
                                                         Ollama (LLM)
```

---

## 1) PrÃ©-requisitos

### Via Docker (recomendado)
- Docker e Docker Compose.
- GPU (opcional): NVIDIA driver instalado no **host** + `gpus: all` no serviÃ§o `rag` do `docker-compose.yml`.

### Ambiente local (sem Docker)
- Python 3.11.
- Sistema:
  - `poppler-utils` (necessÃ¡rio pelo `pdf2image`).
  - `libgl1` e `libglib2.0-0` (runtimes exigidos por OpenCV, mesmo em headless).
- GPU (opcional): CUDA/cuDNN compatÃ­veis com a versÃ£o do PyTorch.

> **Para que servem:**  
> `poppler-utils` fornece `pdftoppm/pdftocairo` usados por `pdf2image` (converter PDF â†’ imagens).  
> `libgl1` e `libglib2.0-0` sÃ£o libs de runtime necessÃ¡rias por OpenCV.

---

## 2) ServiÃ§os externos

- **Ollama** (LLM + embeddings) â€“ e.g., `http://localhost:11435`  
  Instala os modelos pretendidos (os que usei.: `llama3.3:70b-instruct-q2_K`, `snowflake-arctic-embed2:568m`).

- **Qdrant** (Vector DB) â€“ e.g., `http://localhost:6333`  
  
---

## 3) VariÃ¡veis de ambiente (`.env`)

Cria um ficheiro `.env` na raiz:

```dotenv
# ====== Paths / Data ======
RAG_DATA_DIR=/data                     # base de dados local (pdfs/txt)

# ====== Ollama ======
OLLAMA_BASE_URL=http://localhost:11435 # endpoint do Ollama

# ====== LLM da AplicaÃ§Ã£o ======
LLM_MODEL=llama3.3:70b-instruct-q2_K   # LLM para o chat
LLM_NUM_CTX=4096                       # contexto do LLM
LLM_KEEP_ALIVE=10m                     # manter modelo carregado
LLM_TEMPERATURE=0.2                    # criatividade

# ====== Embeddings ======
EMBED_MODEL=snowflake-arctic-embed2:568m # modelo de embeddings (Ollama)

# ====== Qdrant ======
QDRANT_URL=http://localhost:6333       # URL do Qdrant
QDRANT_API_KEY=                        # chave (se necessÃ¡rio)
QDRANT_COLLECTION=docs_pt              # coleÃ§Ã£o de vetores

# ====== Ingest â€“ Perfil/SumÃ¡rio ======
PROFILE_LLM=llama3.3:70b-instruct-q2_K # LLM p/ sumÃ¡rio
PROFILE_LANG=pt-PT                     # idioma
PROFILE_NUM_CTX=8192                   # contexto do LLM de perfil
PROFILE_KEEP_ALIVE=10m                 # keep-alive
MAX_PROFILE_CHARS=40000                # mÃ¡x. chars p/ sumÃ¡rio

# ====== Ingest â€“ Chunking ======
CHUNK_SIZE=800                         # tamanho dos chunks
CHUNK_OVERLAP=150                      # overlap

# ====== Limpeza/OCR ======
FORCE_ALL=0                            # 1=forÃ§ar recriaÃ§Ã£o de TXT

# ====== RecuperaÃ§Ã£o (RAG) ======
TOP_K=5                                # nÂº de chunks "Normal"
MAX_SUMMARIES=2                        # nÂº de itens "summary"

# ====== Servidor ======
RAG_SERVER_HOST=0.0.0.0                # bind
RAG_SERVER_PORT=7860                   # porta (alinhada com compose)
```

---

## 4) InstalaÃ§Ã£o & ExecuÃ§Ã£o

### 4.1. Docker (GPU)


**Subir os serviÃ§os:**
```bash
docker compose up --build -d
```

**Aceder:**  
`http://localhost:7860`

---

### 4.2. ExecuÃ§Ã£o local (sem Docker)

**Sistema:**
```bash
sudo apt-get update
sudo apt-get install -y poppler-utils libgl1 libglib2.0-0
```

**Python:**
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

**Ambiente:** configura `.env` e garante acesso a Ollama e Qdrant.

---

## 5) Pipeline de dados (primeira utilizaÃ§Ã£o)

### 5.1. Preparar PDFs
- Colocar os PDFs em `./data/Original/` (ou conforme `RAG_DATA_DIR`).

### 5.2. Limpeza/OCR â†’ `.txt`
```bash
# Converte PDFs em TXT limpos (OCR quando necessÃ¡rio)
python pdf_clean.py
```

### 5.3. IngestÃ£o / IndexaÃ§Ã£o (a partir dos .txt limpos)
```bash
# LÃª .txt (data/Clean/), cria embeddings via Ollama e escreve no Qdrant
python chatbot/ingest.py
```

> Dicas:
> - Com **GPU**, confirma que o container/ambiente detecta a GPU (`nvidia-smi`).  
> - Sem GPU, assegura que o OCR nÃ£o forÃ§a `use_gpu=True` e que o PyTorch Ã© build CPU.

---

## 6) Arranque do servidor (API/Gradio)

### 6.1. FastAPI (Uvicorn)
```bash
python -m uvicorn rag_app:app --host 0.0.0.0 --port 7860
```
ou 
```bash
python chatbot/rag_app.py server
```

> Garante que `RAG_SERVER_PORT=7860`.

---

## 7) Testes rÃ¡pidos

**Qdrant:**
- `http://localhost:6333/dashboard` (se exposto): confirma a coleÃ§Ã£o `docs_pt`.

**Ollama:**
```bash
curl http://localhost:11435/api/tags
```

**API de exemplo:**
```bash
curl -X POST "http://localhost:7860/ask"   -H "Content-Type: application/json"   -d '{"question": "Qual Ã© o sumÃ¡rio dos documentos carregados?"}'
```

---

## 8) SoluÃ§Ã£o de problemas

- **`pdftoppm not found`** â†’ instalar `poppler-utils`.  
- **OpenCV `libGL.so` missing`** â†’ instalar `libgl1`.  
- **GLib warnings** â†’ instalar `libglib2.0-0`.  
- **GPU nÃ£o detetada no container** â†’ `gpus: all` no compose e `nvidia-container-toolkit` no host.  
- **Torch em CPU apesar de GPU** â†’ garantir wheels **CUDA (cu121)** ou usar imagem PyTorch com CUDA.  
- **Portas** â†’ `RAG_SERVER_PORT` deve coincidir com o mapeamento do compose (7860).

---

## 9) Estrutura de pastas (Ã‰ necessÃ¡rio alterar)
.
â”œâ”€ chatbot
    â”œâ”€ pdf_clean.py    # pdf -> txt
    â”œâ”€ ingest.py       # data do Vector DB
    â”œâ”€ rag_app.py      # LLM, Prompt, similarity search and API/UI
    â”œâ”€ Dockerfile
â””â”€ data/
  â”œâ”€ Original/    # PDFs de origem
  â”œâ”€ Clean/       # TXT limpos/extraÃ­dos
â”œâ”€ requirements.txt
â”œâ”€ docker-compose.yml
â”œâ”€ .env
```

---

## 10) NOTAS
- **Chunking**: `CHUNK_SIZE=800` e `CHUNK_OVERLAP=150` - ajustÃ¡vel como as restantes variÃ¡veis em .env; 
- **Embeddings**: `snowflake-arctic-embed2:568m` 
- **LLM**: `llama3.3:70b-instruct-q2_K`
- Tudo corre **offline** (Ollama + Qdrant locais).

---

