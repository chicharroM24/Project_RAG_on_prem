version: "3.8"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11435:11434"  # host:container
    environment:
      - PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
      - LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_NUM_PARALLEL=2
    volumes:
      - /usr/share/ollama/.ollama:/root/.ollama
    gpus: all
    networks:
      - openwebui-net
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:11434/api/tags >/dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 20s

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"   # REST
      - "6334:6334"   # gRPC
    environment:
      - QDRANT__TELEMETRY_DISABLED=true
    volumes:
      - ./qdrant_storage:/qdrant/storage
    networks:
      - openwebui-net
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:6333/readyz >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s

  rag:
    build:
      context: ./chatbot          # precisa de existir chatbot/Dockerfile
      dockerfile: Dockerfile
    container_name: rag
    restart: unless-stopped
    depends_on:
      - qdrant
      - ollama
    env_file: .env
    volumes:
      - ./chatbot:/app            # c√≥digo Python
      - ./data:/data              # PDFs em /data/pdfs
    gpus: all
    ports:
      - "7860:7860"               # reservado para o Gradio mais tarde
    networks:
      - openwebui-net    
    command: ["bash", "-lc", "sleep infinity"]

networks:
  openwebui-net:
    external: true
